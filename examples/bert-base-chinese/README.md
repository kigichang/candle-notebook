# ä»¥ google-bert/bert-base-chinese ç‚ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ Huggingface ä¸Šçš„é è¨“ç·´æ¨¡å‹

## 1. bert-base-chinese ç°¡ä»‹

ç°¡å–®ä¾†èªªï¼Œæ­¤æ¨¡å‹æ˜¯åœ¨ç©å…‹æ¼å­—å¡«ç©ºçš„éŠæˆ²ï¼Œå°‡å¥å­—ä¸­çš„ __[MASK]__ éƒ¨åˆ†å¡«ä¸Šæ­£ç¢ºçš„å­—ã€‚æ­¤ç¯„ä¾‹å°‡ä¾[ç¶²ç«™](https://huggingface.co/google-bert/bert-base-chinese)ä¸Šçš„ä¾‹å¥ï¼Œç”¢ç”Ÿä¸€æ¨£çš„çµæœã€‚

åŸ·è¡Œæ–¹å¼ï¼š

```bash
$ cargo run --release  --example bert-base-chinese

Input: å·´é»æ˜¯[MASK]å›½çš„é¦–éƒ½ã€‚
"æ³•": 0.991
"å¾·": 0.003
"è‹±": 0.002
"ç¾": 0.001
"è¯¥": 0.001
Input: ç”Ÿæ´»çš„çœŸè°›æ˜¯[MASK]ã€‚
"ç¾": 0.341
"çˆ±": 0.229
"ä¹": 0.033
"äºº": 0.023
"ï¼š": 0.019
```

### 1.1 æº–å‚™å·¥ä½œ

__bert-base-chinese.py__ æ˜¯ Pytorch ç‰ˆæœ¬ç¨‹å¼ï¼Œæ­¤ç¯„ä¾‹å°‡ __bert-base-chinese.py__ï¼Œç§»æ¤åˆ° Candle ä¸Šã€‚ç”±æ–¼å®˜æ–¹ä¸Šçš„æ¨¡å‹æª”æ¡ˆæ˜¯èˆŠçš„æ ¼å¼ï¼Œç›®å‰ Candle ä¸æ”¯æ´ï¼Œå› æ­¤éœ€è¦å°‡æ¨¡å‹è½‰æˆæ–°æ ¼å¼ã€‚__bert-base-chinese.py__ ä¸­æœ‰ä¸€æ®µç¨‹å¼ç¢¼ï¼Œå¯ä»¥å°‡æ¨¡å‹ä¿®æ­£æˆæ–°æ ¼å¼ã€‚

```python
model = AutoModelForMaskedLM.from_pretrained("bert-base-chinese")
model.eval()
output_stat_dict = model.state_dict()
for key in output_stat_dict:
    if "beta" in key or "gamma" in key:
        print("warning: old format name:", key)
torch.save(output_stat_dict, "fix-bert-base-chinese.pth")
```

å†åˆ©ç”¨ Candle åŠŸèƒ½ï¼Œè®€å– Pytorch æ¨¡å‹æª”æ¡ˆå¾Œï¼Œå°‡æ¯ä¸€å€‹æ¬Šé‡æ•´ç†æˆ `HashMap`ï¼Œæœ€å¾Œå†å­˜æˆ `safetensors` æ ¼å¼ã€‚

```rust
fn conv_pth_to_safetensor() -> Result<()> {
    let pth_vec = candle_core::pickle::read_all("fix-bert-base-chinese.pth")?;
    for item in &pth_vec {
        println!("{:?}", item.0);
    }
    let mut tensor_map = HashMap::new();

    for item in pth_vec {
        tensor_map.insert(item.0, item.1);
    }

    candle_core::safetensors::save(&tensor_map, "fix-bert-base-chinese.safetensors")?;
    Ok(())
}
```

ğŸ‘‰ ç¯„ä¾‹ç¨‹å¼ï¼š[conv_tensor.rs](../../tests/conv_tensor.rs)

æˆ‘å·²ç¶“å°‡è½‰æ›å¥½çš„æ¨¡å‹æª”æ¡ˆæ”¾åœ¨ [kigichang/fix-bert-base-chinese](https://huggingface.co/kigichang/fix-bert-base-chinese) ä¸Šï¼Œç¨‹å¼å°‡å¾é€™é‚Šä¸‹è¼‰æ¨¡å‹æª”æ¡ˆã€‚

ç‚ºä»€éº¼ä¸ä½¿ç”¨[å®˜æ–¹çš„å·¥å…·](https://huggingface.co/spaces/safetensors/convert)ï¼Ÿå› ç‚º bert-base-chinese çš„æ¨¡å‹ä¸­ï¼Œæœ‰ [Shared Tensor](https://huggingface.co/docs/safetensors/torch_shared_tensors)ï¼Œå®˜æ–¹çš„å·¥å…·åªæœƒä¿ç•™ç¬¬ä¸€å€‹ key å€¼ï¼Œå…¶ä»– key å€¼æœƒè¢«åˆªé™¤ï¼Œé€ æˆåœ¨è¼‰å…¥æ¨¡å‹æ™‚å‡ºç¾éŒ¯èª¤ã€‚

## 2. ç¨‹å¼èªªæ˜

### 2.1 ä¸‹è¼‰æ¨¡å‹

ä½¿ç”¨ Huggingface æä¾›çš„ `hf_hub` è‡ª Huggingface ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆã€‚ä¸‹è¼‰çš„æ¨¡å‹æª”æ¡ˆåŒ…å«ä¸‰å€‹æª”æ¡ˆï¼š`config.json`ã€`tokenizer.json`ã€`fix-bert-base-chinese.safetensors`ã€‚ä¾æˆ‘çš„ Macbook Pro ç’°å¢ƒï¼Œæª”æ¡ˆæœƒæ”¾åœ¨ `~/.cache/huggingface/hub/models--kigichang--fix-bert-base-chinese` ç›®éŒ„ä¸‹ã€‚å¦‚æœå·²ç¶“ä¸‹è¼‰éï¼Œå‰‡ä¸æœƒå†ä¸‹è¼‰ã€‚

```rust
let default_model = "kigichang/fix-bert-base-chinese".to_string();
let default_revision = "main".to_string();
let repo = Repo::with_revision(default_model, RepoType::Model, default_revision);

let (config_filename, tokenizer_filename, model_filename) = {
    let api = Api::new()?;
    let api = api.repo(repo);
    let config = api.get("config.json")?;
    let tokenizer = api.get("tokenizer.json")?;
    let model = api.get("fix-bert-base-chinese.safetensors")?;
    (config, tokenizer, model)
};
```

Huggingface æä¾› Git çš„æ–¹å¼ç®¡ç†æ¨¡å‹ï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨ `Repo::with_revision` ä¾†æŒ‡å®šæ¨¡å‹çš„ç‰ˆæœ¬ã€‚ç›®å‰ä½¿ç”¨ `main` ä½œç‚ºç‰ˆæœ¬ã€‚

### 2.2 è¼‰å…¥æ¨¡å‹

#### 2.2.1 è¼‰å…¥ Tokenizer

ä½¿ç”¨ Huggingface æä¾›çš„ `tokenizers` è¼‰å…¥æ¨¡å‹ä½¿ç”¨çš„ Tokenizerã€‚Python ç‰ˆæœ¬çš„ Tokenizer åº•å±¤å°±æ˜¯ä½¿ç”¨é€™å€‹å¥—ä»¶ã€‚

```rust
let tokenizer = Tokenizer::from_file(tokenizer_filename).map_err(E::msg)?;
```

#### 2.2.2 è¼‰å…¥ bert-base-chinese æ¨¡å‹

é¦–å…ˆå°‡ JSON æ ¼å¼çš„è¨­å®šæª”ï¼Œè¼‰å…¥æˆ `bert::Config`ã€‚

```rust
let config = File::open(config_filename)?;
Ok(serde_json::from_reader(config)?)
```

ä½¿ç”¨ `VarBuilder` è¼‰å…¥ __safetensors__ æ¨¡å‹æª”æ¡ˆã€‚

```rust
let vb =
        unsafe { VarBuilder::from_mmaped_safetensors(&[model_filename], DType::F32, &device)? };
```

è¼‰å…¥ __safetensors__ æ ¼å¼ï¼Œè¨˜å¾—è¦åŠ ä¸Š `unsafe` é—œéµå­—ã€‚å¦‚è¦ä½¿ç”¨ Pytorch æ¨¡å‹ï¼Œå¯ä»¥åƒè€ƒä¸‹é¢çš„ç¨‹å¼ç¢¼ã€‚

```rust
let vb = VarBuilder::from_pth(model_filename, DType::F32, &device)?;
```

æœ€å¾Œåˆ©ç”¨ `VarBuilder` å»ºç«‹æˆ‘å€‘è¦ä½¿ç”¨çš„ `BertForMaskedLM` æ¨¡å‹ã€‚

```rust
let bert = BertForMaskedLM::load(vb, &config)?;
```

### 2.3 æ¨è«– (Inference)

é¦–å…ˆæˆ‘å€‘å°‡è¼¸å…¥çš„å¥å­ï¼Œç”¢ç”Ÿå°æ‡‰çš„ ID (`input_ids`)ã€‚ç”±æ–¼ `BertForMaskedLM` æœƒéœ€è¦è¼¸å…¥ä¸€å€‹ 2D Tensorï¼Œå› æ­¤æˆ‘å€‘ä½¿ç”¨ `Tensor::stack` å°‡ `input_ids` è½‰æˆ 2D Tensorã€‚

```rust
let ids = tokenizer.encode(test_str, true).map_err(E::msg)?;
let input_ids = Tensor::stack(&[Tensor::new(ids.get_ids(), &device)?], 0)?;
```

æ¥è‘—ç”¢ç”Ÿæ¨è«–æœƒç”¨åˆ°çš„ `type_ids` èˆ‡ `attention_mask`ã€‚èˆ‡ `input_ids` ä¸€æ¨£ï¼Œæˆ‘å€‘ä¹Ÿä½¿ç”¨ `Tensor::stack` å°‡é€™å…©å€‹ Tensor è½‰æˆ 2D Tensorã€‚

```rust
let token_type_ids = Tensor::stack(&[Tensor::new(ids.get_type_ids(), &device)?], 0)?;
let attention_mask = Tensor::stack(&[Tensor::new(ids.get_attention_mask(), &device)?], 0)?;
```

æœ€å¾Œä½¿ç”¨ `foward` å‡½æ•¸ï¼Œé€²è¡Œæ¨è«–ã€‚èˆ‡ä¹‹å‰æåˆ°çš„ `Module` ä¸åŒï¼Œ`forward` å‡½æ•¸åªæ˜¯ç‚ºäº†å°æ‡‰ Pytorch çš„ `forward` å‡½æ•¸ã€‚ä½† Rust ç•¢ç«Ÿä¸æ˜¯åƒ Python ä¸€æ¨£çš„å‹•æ…‹èªè¨€ï¼Œå› æ­¤æ²’æœ‰è¾¨æ³•åƒ Pytorch å¯¦ä½œ `Module` ä¸€æ¨£çš„åŠŸèƒ½ã€‚

```rust
let result = bert.forward(&input_ids, &token_type_ids, Some(&attention_mask))?;
```

### 2.4 çµæœ

åœ¨çµæœçš„å¼µé‡ä¸Šï¼Œå–å¾— `[MASK]` çš„ä½ç½®çš„å¼µé‡çµæœï¼Œä¸¦è¨ˆç®— `softmax` å³ç‚ºæ¯å€‹å­—çš„æ©Ÿç‡ã€‚

```rust
let mask_id: u32 = tokenizer.token_to_id("[MASK]").unwrap();
...

let mask_idx = ids.get_ids().iter().position(|&x| x == mask_id).unwrap();
let mask_token_logits = result.i((0, mask_idx, ..))?;
let mask_token_probs = softmax(&mask_token_logits, 0)?;
```

æœ€å¾Œå–å‡ºå‰ 5 å€‹æ©Ÿç‡æœ€é«˜çš„å­—ï¼Œä½¿ç”¨ `tokenizer.id_to_token` å–å¾—å°æ‡‰çš„å­—ã€‚

```rust
let mut top5_tokens: Vec<(usize, f32)> = mask_token_probs
    .to_vec1::<f32>()?
    .into_iter()
    .enumerate()
    .collect();
top5_tokens.sort_by(|a, b| b.1.total_cmp(&a.1));
let top5_tokens = top5_tokens.into_iter().take(5).collect::<Vec<_>>();

println!("Input: {}", test_str);
for (idx, prob) in top5_tokens {
    println!(
        "{:?}: {:.3}",
        tokenizer.id_to_token(idx as u32).unwrap(),
        prob
    );
}
```

## 3. å¾©ç›¤

1. ä½¿ç”¨ Pytorch ä¿®æ­£æ¨¡å‹æª”æ¡ˆã€‚
1. ä½¿ç”¨ Huggingface çš„ `hf_hub` ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆã€‚
1. ä½¿ç”¨ `VarBuilder` è¼‰å…¥ Pytorch æ¨¡å‹ã€‚
1. ä½¿ç”¨ `Tokenizer` è¼‰å…¥ Tokenizerã€‚
1. ä½¿ç”¨ `BertForMaskedLM` é€²è¡Œæ¨è«–ã€‚
1. ä½¿ç”¨ `softmax` è¨ˆç®—æ©Ÿç‡ã€‚
1. ä½¿ç”¨ `tokenizer.id_to_token` å–å¾—å­—ã€‚
